{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('car_links', skiprows=1) #Indlæser CSV fil der indeholder links\n",
    "features_to_check = [\n",
    "    'GPS navigation', 'Aircon/Klima', 'Anhængertræk', '4-hjulstræk', 'Parkeringssensor',\n",
    "    'Fartpilot', 'ABS', 'Alarm', 'Alufælge', 'Antispin', 'Bluetooth', 'Centrallås',\n",
    "    'ESP', 'El-ruder', 'El-spejle', 'El-sæder', 'Fuldlæder', 'Glastag', 'Headup display',\n",
    "    'Isofix', 'Klimaanlæg', 'Kurvelys', 'Kørecomputer', 'Multifunktionsrat', 'Regnsensor',\n",
    "    'Servo', 'Soltag', 'Sportspakke', 'Sportsæder', 'Stop&Go', 'Sædevarme', 'Vejbane advarselssystem',\n",
    "    'Xenon lygter', 'Ikke-ryger', 'Én-ejers', 'Service OK', 'Nysynet', 'Demo Bil', 'Sportssæder',\n",
    "    'Dellæder', 'CVR/Engros', 'Fuldlæder', 'Adaptiv fartpilot', 'Automatgear', 'Bakkamera', 'Blindvinkelassistent',\n",
    "    'Digital instrumentering', 'Elektrisk bagklap'] # De features vi vil tjekke om bilerne har\n",
    "\n",
    "car_data = [] # Laver en tom liste til at gemme data i\n",
    "\n",
    "feature_presence = {feature: 'No' for feature in features_to_check} #Vi laver en dictionary med features og sætter dem til 'No' som default\n",
    "\n",
    "for url in links[links.columns[0]].values: #Vi looper igennem alle links i vores CSV fil\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    car_headers = soup.find('div', class_='car-specs__pictures-and-car-data-container') # Søger efter den frame hvor den \"primære\" data er\n",
    "    car_row = [] # Resetter vores array\n",
    "    model = soup.find('h1', class_='car-headline').get_text().strip() #Finder der hvor modelnavnet står\n",
    "    car_row.append(f\"{model}\") # Gemmer modelnavnet\n",
    "    if car_headers:\n",
    "        datas = car_headers.findChildren('div', class_='car-data__group-info')\n",
    "        for data in datas: # Looper igennem den \"primære\" data og appender det vi finder til vores array\n",
    "            span_text = data.find('span')\n",
    "            if span_text:\n",
    "                model_text = span_text.get_text().strip() \n",
    "                car_row.append(model_text)\n",
    "    sales_price = soup.find('span', class_='price right').get_text().strip() # Finder salgsprisen\n",
    "    numbers = re.findall(r'\\d+', sales_price) # Splitter teksten op så der ikke står kr. foran\n",
    "    cleaned_price = '.'.join(numbers) # Joiner tallene\n",
    "    car_row.append(f\"{cleaned_price}\") # Gemmer tallene\n",
    "    car_equipment_container = soup.find('div', class_='car-specs__equipment-container')\n",
    "    if car_equipment_container:\n",
    "        equipment_lists = car_equipment_container.find_all('ul', class_='equipment-list') # Finder der hvor alle features står\n",
    "        for equipment_list in equipment_lists:\n",
    "            list_items = equipment_list.find_all('li')\n",
    "            for item in list_items:\n",
    "                span = item.find('span')\n",
    "                if span and 'Farve:' in span.get_text(): #Tjekker og gemmer farven\n",
    "                    color = item.get_text().split(':')[1].strip()\n",
    "                    car_row.append(color) \n",
    "                for feature in features_to_check: \n",
    "                    if feature in item.get_text():\n",
    "                        feature_presence[feature] = 'Yes' # Hvis featuren er i listen, så sætter vi den til Yes frem for No\n",
    "        for feature, presence in feature_presence.items():\n",
    "            car_row.append(f\"{presence}\") # Gemmer om bilen har featuren eller ej\n",
    "    car_row.append(url) # Gemmer URL'en så vi har noget at linke tilbage til hvis der skulle være noget galt med dataen\n",
    "    np.transpose(car_row) # Flipper dataen så det står vandret frem for lodret\n",
    "    car_data.append(car_row) # Gemmer til vores tomme liste vi lavede i starten\n",
    "    time.sleep(4) # Sleep timer så vi ikke får too many requests/timeout\n",
    "df = pd.DataFrame(car_data) #Vi gemmer vores liste i en dataframe\n",
    "df.to_csv('car_data', index=False)  # Gemmer vores csv fil når loopet er done, tog ca. 24 timer at køre igennem 20.000 links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
